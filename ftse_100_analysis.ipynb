{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "from datetime import date, datetime, timedelta\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert row ftse file data to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ToDo: move all methods to a separate file and import them\n",
    "\n",
    "def convert_txt_to_dict(filename):\n",
    "    content = []\n",
    "\n",
    "    with open(filename, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            content.append(row)\n",
    "\n",
    "    data_ftse_100_dict = {}\n",
    "    tickers_list = [ticker[:-2] for ticker in content[0][1:]]\n",
    "    data_ftse_100_dict = {}\n",
    "\n",
    "    for row in content[1:]:\n",
    "        data_ftse_100_dict[row[0]] = {}\n",
    "        for i, ticker in enumerate(tickers_list):\n",
    "            data_ftse_100_dict[row[0]][ticker] = row[i + 1]\n",
    "    return data_ftse_100_dict\n",
    "\n",
    "    \n",
    "txt_filename = 'ftse_100_time_data.csv'\n",
    "path_to_save = \"./data_ftse_interp_0_1_2001_2006.p\"\n",
    "\n",
    "data_ftse_100_dict = convert_txt_to_dict(txt_filename)\n",
    "\n",
    "# pickle.dump(data_ftse_100_dict, open(path_to_save, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library to interpolate ftse dataset\n",
    "\n",
    "How to write `add_dates` method\n",
    "http://stackoverflow.com/questions/3240458/how-to-increment-the-day-in-datetime-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datetime_to_date(datetime_date):\n",
    "    datetime_date.date().year\n",
    "    str_date = '{}-{}-{}'.format(datetime_date.date().strftime('%Y'), datetime_date.date().strftime('%m'), datetime_date.date().strftime('%d'))\n",
    "    return str_date\n",
    "    \n",
    "def date_to_datetime(date):\n",
    "    return datetime.strptime(date, '%Y-%m-%d')\n",
    "\n",
    "def find_start_end_dates(ticker, dates_dict):\n",
    "    # this method looks when a company started to be in ftse 100. Assume that some dates may not have dictionaries\n",
    "    # with ticker. No ticker in keys.\n",
    "    start = \"\"\n",
    "    end = \"\"\n",
    "    for date, stock_values in dates_dict.items():\n",
    "        if ticker in stock_values.keys():\n",
    "            if stock_values[ticker]:\n",
    "                # check if start is empty\n",
    "                if not start:\n",
    "                    start = date\n",
    "                else:\n",
    "                    end = date\n",
    "    \n",
    "    return start, end # dates in datetime format\n",
    "\n",
    "def get_dates_range(start_date, end_date):\n",
    "    # assume start_date and end_date are in datetime format\n",
    "    dates_range = []\n",
    "    delta = end_date - start_date\n",
    "    date_index = start_date\n",
    "    for i in range(delta.days + 1):\n",
    "        dates_range.append(date_index)\n",
    "        date_index += timedelta(days=1)\n",
    "    return dates_range\n",
    "\n",
    "def add_interpolated_values(dates_range, ticker, dates_dict):\n",
    "    modified_dates_dict = dates_dict\n",
    "    start_indicator = dates_range[0]\n",
    "    end_indicator = dates_range[-1]\n",
    "    interpolated_value = (float(modified_dates_dict[datetime_to_date(start_indicator)][ticker]) + float(modified_dates_dict[datetime_to_date(end_indicator)][ticker])) / 2\n",
    "    for date in dates_range[1:-1]:\n",
    "        # Initialise new date\n",
    "        try:\n",
    "            value = modified_dates_dict[datetime_to_date(date)]\n",
    "        except KeyError:\n",
    "            modified_dates_dict[datetime_to_date(date)] = {}\n",
    "        \n",
    "        modified_dates_dict[datetime_to_date(date)][ticker] = interpolated_value\n",
    "\n",
    "    return modified_dates_dict # add average values using start_indicator and end_indicator\n",
    "    \n",
    "def add_missing_date(missing_date, complete_dates_dict, ticker):\n",
    "    #print(\"here\")\n",
    "    #print(len(list(complete_dates_dict.keys())))\n",
    "    try:\n",
    "        dic = complete_dates_dict[missing_date]\n",
    "        try:\n",
    "            value = complete_dates_dict[missing_date][ticker]\n",
    "        except KeyError:\n",
    "            complete_dates_dict[missing_date][ticker] = \"\"\n",
    "    except KeyError:\n",
    "        complete_dates_dict[missing_date] = {}\n",
    "        complete_dates_dict[missing_date][ticker] = \"\"\n",
    "\n",
    "def initialize_date(date_init, all_dates_dict, ticker, initial_value):\n",
    "    if not all_dates_dict[datetime_to_date(date_init)][ticker]:\n",
    "        all_dates_dict[datetime_to_date(date_init)][ticker] = initial_value\n",
    "\n",
    "def interpolate(dates_dict, ticker, start_date, end_date):\n",
    "    # assume start_date and end_date are in datetime format\n",
    "    dates_range = get_dates_range(start_date, end_date)\n",
    "    start_indicator = dates_range[0]\n",
    "    dates_dict_interp = dates_dict\n",
    "    \n",
    "    add_missing_date(datetime_to_date(start_indicator), dates_dict_interp, ticker)\n",
    "    initialize_date(start_indicator, dates_dict_interp, ticker, 500)\n",
    "    \n",
    "    for date in dates_range[1:]:\n",
    "        add_missing_date(datetime_to_date(date), dates_dict_interp, ticker)\n",
    "        #print(\"start\", datetime_to_date(start_indicator))\n",
    "        #print(\"end\", datetime_to_date(end_indicator))\n",
    "        #print(\"date\", datetime_to_date(date))\n",
    "        #print(\"ticker_value\", dates_dict_interp[datetime_to_date(date)][ticker])\n",
    "        #print(dates_dict_interp[datetime_to_date(date)][ticker])\n",
    "        if dates_dict_interp[datetime_to_date(date)][ticker]:\n",
    "            #print(dates_dict_interp[datetime_to_date(date)][ticker])\n",
    "            #print(datetime_to_date(date))\n",
    "            #print(\"empty\")\n",
    "            if (date - start_indicator).days >= 1:\n",
    "                dates_range_interp = get_dates_range(start_indicator, date)\n",
    "                dates_dict_interp = add_interpolated_values(dates_range_interp, ticker, dates_dict_interp)\n",
    "                start_indicator = date\n",
    "            else:\n",
    "                start_indicator = date\n",
    "            \n",
    "    return dates_dict_interp\n",
    "\n",
    "def get_existing_companies(date_point, data_ftse_100):\n",
    "    tickers_list = list(data_ftse_100['2012-07-11'].keys())\n",
    "    valid_tickers = []\n",
    "    for ticker in tickers_list:\n",
    "        start, end = find_start_end_dates(ticker, data_ftse_100)\n",
    "        if date_to_datetime(date_point) > date_to_datetime(start):\n",
    "            valid_tickers.append(ticker)\n",
    "    return valid_tickers\n",
    "\n",
    "def interpolate_all(tickers_lst, data_ftse, date_init, date_end):\n",
    "    data_ftse_100_dict_interp = copy.deepcopy(data_ftse) \n",
    "    for ticker in tickers_lst:\n",
    "        data_ftse_100_dict_interp = interpolate(data_ftse_100_dict_interp, ticker, date_to_datetime(date_init), date_to_datetime(date_end))\n",
    "    \n",
    "    return data_ftse_100_dict_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1993-03-31 2015-04-29\n"
     ]
    }
   ],
   "source": [
    "start, end = find_start_end_dates(\"III\", data_ftse_100_dict)\n",
    "print(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Calculate correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.0503901   0.10666312 -0.00988581]\n",
      " [ 0.0503901   1.          0.44133805  0.39424062]\n",
      " [ 0.10666312  0.44133805  1.          0.2825074 ]\n",
      " [-0.00988581  0.39424062  0.2825074   1.        ]]\n"
     ]
    }
   ],
   "source": [
    "def get_components_matrix(time_window, time_start, time_end, ticker_list, dates_dict):\n",
    "    components_distance = {}\n",
    "    dates_range = get_dates_range(date_to_datetime(time_start), date_to_datetime(time_end))\n",
    "    \n",
    "    for date in dates_range:\n",
    "        vectors = []\n",
    "        for ticker in ticker_list:\n",
    "            vectors.append(get_s_vector(time_window, ticker, dates_dict, date))\n",
    "        components_distance[datetime_to_date(date)] = np.corrcoef(vectors)\n",
    "    return components_distance # dict where each key is time and value is distance\n",
    "\n",
    "def get_s_vector(time_window, ticker, dates_dict, initial_time):\n",
    "    s_vector = []\n",
    "    end_time = initial_time + timedelta(days=time_window)\n",
    "    dates_window = get_dates_range(initial_time, end_time)\n",
    "    for date in dates_window:\n",
    "        s_vector.append(get_s(date, ticker, dates_dict))\n",
    "    return s_vector\n",
    "\n",
    "def get_s(time, ticker, dates_dict):\n",
    "    s = np.log(float(dates_dict[datetime_to_date(time + timedelta(days=1))][ticker])) - np.log(float(dates_dict[datetime_to_date(time)][ticker]))\n",
    "    return s\n",
    "\n",
    "ticker_1 = valid_tickers[5:][0]\n",
    "ticker_2 = valid_tickers[5:7][1]  \n",
    "#ticker_1 = \"ARM\"\n",
    "#ticker_2 = \"ANTO\" \n",
    "#print(np.log(float(data_ftse_100_dict_interp[datetime_to_date(date_to_datetime('2013-04-01') + timedelta(days=1))][ticker])))\n",
    "dist = get_components_matrix(45, '2012-07-13', '2013-07-15', valid_tickers[0:4], data_ftse_interp)\n",
    "print(dist['2012-07-13'])\n",
    "# numpy.corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IGRAPH U--- 4 12 -- \\n+ attr: tickers (v), correlation (e)'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import igraph\n",
    "\n",
    "def edges_vector(tickers, matrix):\n",
    "    edges = []\n",
    "    edges_values = []\n",
    "    for i, tick_i in enumerate(tickers):\n",
    "        for j, tick_j in enumerate(tickers):\n",
    "            if j != i:\n",
    "                edges.append((i, j))\n",
    "                edges_values.append(matrix[i][j])\n",
    "                \n",
    "    return edges, edges_values\n",
    "\n",
    "def create_graph(tickers, matrix):\n",
    "    edges_lst, edges_corr = edges_vector(tickers, matrix)\n",
    "    g = igraph.Graph(edges_lst)\n",
    "    g.vs[\"tickers\"] = tickers\n",
    "    g.es[\"correlation\"] = edges_corr\n",
    "    \n",
    "    return g\n",
    "    \n",
    "g_res = create_graph(valid_tickers[0:4], dist['2012-07-13'])\n",
    "    \n",
    "g_res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.050390096061847738, 0.10666312114380493, -0.0098858061980766007, 0.050390096061847738, 0.44133804542643257, 0.394240619048779, 0.10666312114380494, 0.44133804542643257, 0.28250740376875799, -0.0098858061980766007, 0.394240619048779, 0.28250740376875799]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'IGRAPH U--- 4 3 -- \\n+ attr: tickers (v), correlation (e)'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(g_res.es[\"correlation\"])\n",
    "mst = g_res.spanning_tree(weights=g_res.es[\"correlation\"])\n",
    "mst.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAL', 'ABF', 'ADN', 'AGK']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mst.vs[\"tickers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.050390096061847738, 0.10666312114380493, -0.0098858061980766007]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mst.es[\"correlation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1)\n",
      "(0, 2)\n",
      "(0, 3)\n"
     ]
    }
   ],
   "source": [
    "for e in mst.es:\n",
    "    print(e.tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I want to get in an output? For each company I would like to get degree of vertice as a function of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 1, 1]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mst.degree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_degree_distribution(window_size, start_date, end_date, dataset_init):\n",
    "    degrees_dict = {}\n",
    "    \n",
    "    valid_tickers = get_existing_companies(start_date, dataset_init)\n",
    "    \n",
    "    data_ftse_interp = interpolate_all(valid_tickers, dataset_init, start_date, end_date)\n",
    "    \n",
    "    end_date_datetime_win = date_to_datetime(end_date) - timedelta(days=window_size+1)\n",
    "    end_date_win = datetime_to_date(end_date_datetime_win)\n",
    "    \n",
    "    dist_corr = get_components_matrix(window_size, start_date, end_date_win, valid_tickers, data_ftse_interp)\n",
    "    \n",
    "    for date_corr in dist_corr.keys():\n",
    "        g_res = create_graph(valid_tickers, dist_corr[date_corr])\n",
    "        mst = g_res.spanning_tree(weights=g_res.es[\"correlation\"])\n",
    "        degrees_dict[date_corr] = mst.degree()\n",
    "        \n",
    "    return degrees_dict\n",
    "        \n",
    "deg_dict = calculate_degree_distribution(360, '2010-07-14', '2011-10-11', data_ftse_100_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AAL', 1), ('ABF', 1), ('ADM', 1), ('ADN', 1), ('AGK', 1), ('AHT', 1), ('ANTO', 1), ('ARM', 1), ('AV', 1), ('AZN', 1), ('BA', 1), ('BAB', 1), ('BARC', 1), ('BATS', 1), ('BDEV', 1), ('BG', 1), ('BLND', 1), ('BLT', 1), ('BNZL', 1), ('BP', 1), ('BRBY', 1), ('BT-A', 1), ('CCL', 1), ('CNA', 1), ('CPG', 1), ('CPI', 1), ('CRH', 1), ('DC', 1), ('DGE', 1), ('EXPN', 1), ('EZJ', 1), ('FRES', 1), ('GFS', 1), ('GKN', 1), ('GSK', 1), ('HIK', 1), ('HL', 1), ('HMSO', 1), ('HSBA', 1), ('IAG', 1), ('IHG', 1), ('III', 46), ('IMT', 1), ('INTU', 1), ('ITRK', 1), ('ITV', 1), ('JMAT', 1), ('KGF', 1), ('LAND', 1), ('LGEN', 1), ('LLOY', 1), ('LSE', 1), ('MGGT', 1), ('MKS', 1), ('MNDI', 1), ('MRW', 1), ('NG', 1), ('NXT', 1), ('OML', 1), ('PRU', 1), ('PSN', 1), ('PSON', 1), ('RB', 1), ('RBS', 1), ('RDSA', 1), ('RDSB', 1), ('REL', 1), ('RIO', 1), ('RR', 1), ('RRS', 48), ('RSA', 1), ('SAB', 1), ('SBRY', 1), ('SDR', 1), ('SGE', 1), ('SHP', 1), ('SKY', 1), ('SL', 1), ('SMIN', 1), ('SN', 1), ('SPD', 2), ('SSE', 1), ('STAN', 1), ('STJ', 1), ('SVT', 1), ('TPK', 1), ('TSCO', 1), ('TW', 1), ('ULVR', 1), ('UU', 1), ('VOD', 1), ('WEIR', 1), ('WOS', 1), ('WPP', 1), ('WTB', 1)]\n"
     ]
    }
   ],
   "source": [
    "tic_degree = []\n",
    "valid_tickers = get_existing_companies('2010-10-15', data_ftse_100_dict)\n",
    "\n",
    "for i, degree in enumerate(deg_dict['2010-10-15']):\n",
    "    tic_degree.append((valid_tickers[i], degree))\n",
    "    \n",
    "print(tic_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AAL', 1), ('ABF', 1), ('ADM', 1), ('ADN', 1), ('AGK', 1), ('AHT', 1), ('ANTO', 1), ('ARM', 1), ('AV', 1), ('AZN', 1), ('BA', 1), ('BAB', 1), ('BARC', 1), ('BATS', 1), ('BDEV', 1), ('BG', 1), ('BLND', 1), ('BLT', 1), ('BNZL', 1), ('BP', 1), ('BRBY', 1), ('BT-A', 1), ('CCL', 1), ('CNA', 1), ('CPG', 1), ('CPI', 1), ('CRH', 1), ('DC', 1), ('DGE', 1), ('EXPN', 1), ('EZJ', 1), ('FRES', 1), ('GFS', 1), ('GKN', 1), ('GSK', 1), ('HIK', 1), ('HL', 1), ('HMSO', 1), ('HSBA', 1), ('IAG', 1), ('IHG', 1), ('IMT', 1), ('INTU', 1), ('ITRK', 1), ('ITV', 1), ('JMAT', 1), ('KGF', 1), ('LAND', 1), ('LGEN', 1), ('LLOY', 1), ('LSE', 1), ('MGGT', 1), ('MKS', 1), ('MNDI', 1), ('MRW', 1), ('NG', 1), ('NXT', 1), ('OML', 1), ('PRU', 1), ('PSN', 1), ('PSON', 1), ('RB', 1), ('RBS', 1), ('RDSA', 1), ('RDSB', 1), ('REL', 1), ('RIO', 1), ('RR', 1), ('RSA', 1), ('SAB', 1), ('SBRY', 1), ('SDR', 1), ('SGE', 1), ('SHP', 1), ('SKY', 1), ('SL', 1), ('SMIN', 1), ('SN', 1), ('SSE', 1), ('STAN', 1), ('STJ', 1), ('SVT', 1), ('TPK', 1), ('TSCO', 1), ('TW', 1), ('ULVR', 1), ('UU', 1), ('VOD', 1), ('WEIR', 1), ('WOS', 1), ('WPP', 1), ('WTB', 1), ('SPD', 2), ('III', 46), ('RRS', 48)]\n"
     ]
    }
   ],
   "source": [
    "tic_degree.sort(key=itemgetter(1))\n",
    "print(tic_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** Randgold is in the center of the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_dict_2 = calculate_degree_distribution(360, '2012-07-14', '2013-10-11', data_ftse_100_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AAL', 1), ('ABF', 1), ('ADM', 1), ('ADN', 1), ('AGK', 2), ('AHT', 1), ('ANTO', 1), ('ARM', 1), ('AV', 1), ('AZN', 1), ('BA', 1), ('BAB', 1), ('BARC', 1), ('BATS', 1), ('BDEV', 1), ('BG', 1), ('BLND', 1), ('BLT', 1), ('BNZL', 1), ('BP', 1), ('BRBY', 1), ('BT-A', 1), ('CCL', 1), ('CNA', 1), ('CPG', 1), ('CPI', 1), ('CRH', 1), ('DC', 1), ('DGE', 1), ('EXPN', 1), ('EZJ', 1), ('FRES', 1), ('GFS', 1), ('GKN', 1), ('GLEN', 1), ('GSK', 2), ('HIK', 1), ('HL', 1), ('HMSO', 1), ('HSBA', 1), ('IAG', 1), ('IHG', 1), ('III', 1), ('IMT', 1), ('INTU', 1), ('ITRK', 1), ('ITV', 1), ('JMAT', 1), ('KGF', 1), ('LAND', 1), ('LGEN', 1), ('LLOY', 1), ('LSE', 1), ('MGGT', 1), ('MKS', 1), ('MNDI', 1), ('MRW', 1), ('NG', 1), ('NXT', 1), ('OML', 1), ('PRU', 1), ('PSN', 1), ('PSON', 1), ('RB', 1), ('RBS', 1), ('RDSA', 1), ('RDSB', 1), ('REL', 1), ('RIO', 1), ('RR', 92), ('RRS', 2), ('RSA', 1), ('SAB', 1), ('SBRY', 1), ('SDR', 1), ('SGE', 1), ('SHP', 1), ('SKY', 1), ('SL', 1), ('SMIN', 1), ('SN', 1), ('SPD', 1), ('SSE', 1), ('STAN', 1), ('STJ', 1), ('SVT', 1), ('TPK', 1), ('TSCO', 1), ('TW', 1), ('ULVR', 1), ('UU', 1), ('VOD', 1), ('WEIR', 1), ('WOS', 1), ('WPP', 1), ('WTB', 1)]\n",
      "[('AAL', 1), ('ABF', 1), ('ADM', 1), ('ADN', 1), ('AHT', 1), ('ANTO', 1), ('ARM', 1), ('AV', 1), ('AZN', 1), ('BA', 1), ('BAB', 1), ('BARC', 1), ('BATS', 1), ('BDEV', 1), ('BG', 1), ('BLND', 1), ('BLT', 1), ('BNZL', 1), ('BP', 1), ('BRBY', 1), ('BT-A', 1), ('CCL', 1), ('CNA', 1), ('CPG', 1), ('CPI', 1), ('CRH', 1), ('DC', 1), ('DGE', 1), ('EXPN', 1), ('EZJ', 1), ('FRES', 1), ('GFS', 1), ('GKN', 1), ('GLEN', 1), ('HIK', 1), ('HL', 1), ('HMSO', 1), ('HSBA', 1), ('IAG', 1), ('IHG', 1), ('III', 1), ('IMT', 1), ('INTU', 1), ('ITRK', 1), ('ITV', 1), ('JMAT', 1), ('KGF', 1), ('LAND', 1), ('LGEN', 1), ('LLOY', 1), ('LSE', 1), ('MGGT', 1), ('MKS', 1), ('MNDI', 1), ('MRW', 1), ('NG', 1), ('NXT', 1), ('OML', 1), ('PRU', 1), ('PSN', 1), ('PSON', 1), ('RB', 1), ('RBS', 1), ('RDSA', 1), ('RDSB', 1), ('REL', 1), ('RIO', 1), ('RSA', 1), ('SAB', 1), ('SBRY', 1), ('SDR', 1), ('SGE', 1), ('SHP', 1), ('SKY', 1), ('SL', 1), ('SMIN', 1), ('SN', 1), ('SPD', 1), ('SSE', 1), ('STAN', 1), ('STJ', 1), ('SVT', 1), ('TPK', 1), ('TSCO', 1), ('TW', 1), ('ULVR', 1), ('UU', 1), ('VOD', 1), ('WEIR', 1), ('WOS', 1), ('WPP', 1), ('WTB', 1), ('AGK', 2), ('GSK', 2), ('RRS', 2), ('RR', 92)]\n"
     ]
    }
   ],
   "source": [
    "tic_degree_2 = []\n",
    "valid_tickers_2 = get_existing_companies('2012-07-14', data_ftse_100_dict)\n",
    "\n",
    "for i, degree in enumerate(deg_dict_2['2012-07-26']):\n",
    "    tic_degree_2.append((valid_tickers_2[i], degree))\n",
    "    \n",
    "print(tic_degree_2)\n",
    "\n",
    "tic_degree_2.sort(key=itemgetter(1))\n",
    "print(tic_degree_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** Royce Royce in the center of the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
